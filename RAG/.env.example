# RAG System Environment Configuration Example
# ============================================
# Copy this file to .env and update the values according to your setup

# ===== QDRANT VECTOR DATABASE CONFIGURATION =====
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334
# QDRANT_API_KEY=your_qdrant_api_key_here

# ===== LLM API CONFIGURATION =====
# Choose between LOCAL or REMOTE LLM setup

# === LOCAL LLM SETUP (Jan, Ollama, etc.) ===
LOCAL_LLM_API_BASE=http://localhost:1337/v1
LOCAL_LLM_MODEL_NAME=llama3.2:3b
LOCAL_LLM_API_KEY=NotNeeded

# === REMOTE LLM SETUP (RunPod, OpenAI-compatible APIs) ===
REMOTE_LLM_API_BASE=https://api.runpod.ai/v2/YOUR_ENDPOINT_ID/openai/v1
REMOTE_LLM_MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
REMOTE_LLM_API_KEY=your_runpod_api_key_here

# ===== ACTIVE LLM CONFIGURATION =====
# Uncomment one of the following sets:

# FOR LOCAL LLM:
# LLM_API_BASE=http://localhost:1337/v1
# LLM_MODEL_NAME=llama3.2:3b
# LLM_API_KEY=NotNeeded

# FOR REMOTE LLM:
LLM_API_BASE=https://api.runpod.ai/v2/YOUR_ENDPOINT_ID/openai/v1
LLM_MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
LLM_API_KEY=your_runpod_api_key_here

# ===== EMBEDDING MODELS CONFIGURATION =====
DENSE_EMBEDDING_MODEL_NAME=BAAI/bge-base-en-v1.5
SPARSE_EMBEDDING_MODEL_NAME=Qdrant/bm25

# ===== RETRIEVAL CONFIGURATION =====
DEFAULT_RETRIEVAL_TOP_K=3
QDRANT_COLLECTION_NAME=pdf_knowledge_base_hybrid3

# ===== EXTERNAL APIs =====
# STACKEXCHANGE_API_KEY=your_stackexchange_api_key_here

# ===== LOGGING CONFIGURATION =====
LOG_LEVEL=INFO
DEBUG_PROMPTS=false

# ===== DEVELOPMENT/TESTING =====
DEVELOPMENT_MODE=false
LLM_REQUEST_TIMEOUT=30
API_REQUEST_TIMEOUT=10
